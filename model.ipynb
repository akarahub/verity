{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, pipeline, \\\n",
    "    TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde9e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# YAML config\n",
    "try:\n",
    "    with open(r\".\\config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "except Exception as e:\n",
    "    raise\n",
    "\n",
    "# Logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(funcName)s - %(message)s\",\n",
    "    filename=config[\"log_dir\"] +\n",
    "    f\"{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.log\",\n",
    "    filemode=\"w\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Config file and logger setup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af512c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"path\")\n",
    "response = response.text\n",
    "response = response.splitlines()\n",
    "train_tokens = []\n",
    "train_tags = []\n",
    "\n",
    "temp_tokens = []\n",
    "temp_tags = []\n",
    "for line in response:\n",
    "    if line != \"\":\n",
    "        tag, token = line.strip().split(\"\\t\")\n",
    "        temp_tags.append(tag)\n",
    "        temp_tokens.append(token)\n",
    "    else:\n",
    "        train_tokens.append(temp_tokens)\n",
    "        train_tags.append(temp_tags)\n",
    "\n",
    "        temp_tokens, temp_tags = [], []\n",
    "\n",
    "len(train_tokens), len(train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"path\")\n",
    "response = response.text\n",
    "response = response.splitlines()\n",
    "\n",
    "test_tokens = []\n",
    "test_tags = []\n",
    "\n",
    "temp_tokens = []\n",
    "temp_tags = []\n",
    "for line in response:\n",
    "    if line != \"\":\n",
    "        tag, token = line.strip().split(\"\\t\")\n",
    "        temp_tags.append(tag)\n",
    "        temp_tokens.append(token)\n",
    "    else:\n",
    "        test_tokens.append(temp_tokens)\n",
    "        test_tags.append(temp_tags)\n",
    "\n",
    "        temp_tokens, temp_tags = [], []\n",
    "\n",
    "len(test_tokens), len(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "def load_and_preprocess_data(train_tags, train_tokens, test_tags, test_tokens) -> DatasetDict:\n",
    "    \"\"\"Loads training and testing data, performs tokenization and label alignment.\"\"\"\n",
    "    try:\n",
    "        # Load data\n",
    "        # train_tags = []\n",
    "        # with open(train_tags, \"r\") as train_file_tags:\n",
    "        #     train_tags.append(train_file_tags.readlines())\n",
    "        # train_tags = [[i.rstrip() for i in item] for item in train_tags]\n",
    "        # train_tags = [item[i].split() for item in train_tags for i in range(len(train_tags[0]))]\n",
    "\n",
    "        # train_tokens = []\n",
    "        # with open(train_tokens, \"r\") as train_file_tokens:\n",
    "        #     train_tokens.append(train_file_tokens.readlines())\n",
    "        # train_tokens = [[i.rstrip() for i in item] for item in train_tokens]\n",
    "        # train_tokens = [item[i].split() for item in train_tokens for i in range(len(train_tokens[0]))]\n",
    "\n",
    "        # test_tags = []\n",
    "        # with open(test_tags, \"r\") as test_file_tags:\n",
    "        #     test_tags.append(test_file_tags.readlines())\n",
    "        # test_tags = [[i.rstrip() for i in item] for item in test_tags]\n",
    "        # test_tags = [item[i].split() for item in test_tags for i in range(len(test_tags[0]))]\n",
    "\n",
    "        # test_tokens = []\n",
    "        # with open(test_tokens, \"r\") as test_file_tokens:\n",
    "        #     test_tokens.append(test_file_tokens.readlines())\n",
    "        # test_tokens = [[i.rstrip() for i in item] for item in test_tokens]\n",
    "        # test_tokens = [item[i].split() for item in test_tokens for i in range(len(test_tokens[0]))]\n",
    "\n",
    "        # Create Pandas DataFrames\n",
    "        df_train = pd.DataFrame(\n",
    "            {\"tokens\": train_tokens, \"ner_tags_str\": train_tags})\n",
    "        df_test = pd.DataFrame(\n",
    "            {\"tokens\": test_tokens, \"ner_tags_str\": test_tags})\n",
    "\n",
    "        # Convert to Datasets\n",
    "        train = Dataset.from_pandas(df_train)\n",
    "        test = Dataset.from_pandas(df_test)\n",
    "\n",
    "        # Create DatasetDict\n",
    "        dataset = DatasetDict(\n",
    "            {\"train\": train, \"test\": test, \"validation\": test})\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"File not found: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b6a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "def create_label_index(dataset):\n",
    "    \"\"\"Creates a mapping from tag names to integer indices.\"\"\"\n",
    "    try:\n",
    "        unique_tags = set()\n",
    "        for tag in dataset[\"train\"][\"ner_tags_str\"]:\n",
    "            unique_tags.update(tag)\n",
    "\n",
    "        unique_tags = list(set([x[2:] for x in list(unique_tags) if x != \"O\"]))\n",
    "\n",
    "        tag2index = {\"O\": 0}\n",
    "        for i, tag in enumerate(unique_tags):\n",
    "            tag2index[f\"B-{tag}\"] = len(tag2index)\n",
    "            tag2index[f\"I-{tag}\"] = len(tag2index)\n",
    "\n",
    "        index2tag = {v: k for k, v in tag2index.items()}\n",
    "\n",
    "        return tag2index, index2tag\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Label and index creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49780389",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_and_preprocess_data(\n",
    "    train_tags, train_tokens, test_tags, test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ecd5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2index, index2tag = create_label_index(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5a83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda example: {\"ner_tags\": [\n",
    "                      tag2index[tag] for tag in example[\"ner_tags_str\"]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model_checkpoint\"])\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"Tokenizes the input and aligns labels with the token IDs.\"\"\"\n",
    "    try:\n",
    "        tokenized_inputs = tokenizer(\n",
    "            examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "\n",
    "                previous_word_idx = word_idx\n",
    "            labels.append(label_ids)\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Tokenizing failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00229e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b74111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator\n",
    "def create_data_collator(tokenizer):\n",
    "    \"\"\"Creates a data collator for the token classification model.\"\"\"\n",
    "    return DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Calculation\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Computes evaluation metrics for the token classification model.\"\"\"\n",
    "    try:\n",
    "        metric = evaluate.load(\"seqeval\")\n",
    "        label_names = list(tag2index)\n",
    "\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        true_labels = [[label_names[l]\n",
    "                        for l in label if l != -100] for label in labels]\n",
    "        true_predictions = [[label_names[p] for p, l in zip(prediction, label) if l != -100]\n",
    "                            for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "        all_metrics = metric.compute(\n",
    "            predictions=true_predictions, references=true_labels)\n",
    "\n",
    "        return {\n",
    "            \"precision\": all_metrics[\"overall_precision\"],\n",
    "            \"recall\": all_metrics[\"overall_recall\"],\n",
    "            \"f1\": all_metrics[\"overall_f1\"],\n",
    "            \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Evaluation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e40fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    config[\"model_checkpoint\"], id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d9c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = create_data_collator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e20945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f8ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipe(model_path: str):\n",
    "    \"\"\"\n",
    "    Make prediction over custom text.\n",
    "\n",
    "    Args:\n",
    "        model_path: Path for trained model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pipe = pipeline(\"token-classification\", model=model_path,\n",
    "                        aggregation_strategy=\"simple\")\n",
    "        return pipe\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Custom prediction failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eeeafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = create_pipe(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e368f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"query\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
